{{- /*gotype: github.com/hashicorp/terraform-plugin-docs/internal/provider.ResourceTemplateType */ -}}
---
subcategory: "Kubernetes"
page_title: "Scaleway: scaleway_k8s_cluster"
---

# Resource: scaleway_k8s_cluster

{{ .Description }}

{{ if .HasExamples }}
## Example Usage

{{ range .ExampleFiles -}}
{{ tffile . }}

{{ end }}


{{ end -}}

## Argument Reference

The following arguments are supported:

- `name` - (Required) The name for the Kubernetes cluster.

- `type` - (Optional) The type of Kubernetes cluster. Possible values are:

    - for mutualized clusters: `kapsule` or `multicloud`

    - for dedicated Kapsule clusters: `kapsule-dedicated-4`, `kapsule-dedicated-8` or `kapsule-dedicated-16`.

    - for dedicated Kosmos clusters: `multicloud-dedicated-4`, `multicloud-dedicated-8` or `multicloud-dedicated-16`.

- `description` - (Optional) A description for the Kubernetes cluster.

- `version` - (Required) The version of the Kubernetes cluster.

- `cni` - (Required) The Container Network Interface (CNI) for the Kubernetes cluster.
~> **Important:** Updates to this field will recreate a new resource.

- `delete_additional_resources` - (Required) Delete additional resources like block volumes, load-balancers and the cluster's private network (if empty) that were created in Kubernetes on cluster deletion.
~> **Important:** Setting this field to `true` means that you will lose all your cluster data and network configuration when you delete your cluster.
If you prefer keeping it, you should instead set it as `false`.

- `private_network_id` - (Required) The ID of the private network of the cluster.

~> **Important:** Changes to this field will recreate a new resource.

~> **Important:** Private Networks are now mandatory with Kapsule Clusters. If you have a legacy cluster (no `private_network_id` set),
you can still set it now. In this case it will not destroy and recreate your cluster but migrate it to the Private Network.

- `tags` - (Optional) The tags associated with the Kubernetes cluster.

- `autoscaler_config` - (Optional) The configuration options for the [Kubernetes cluster autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler).

    - `disable_scale_down` - (Defaults to `false`) Disables the scale down feature of the autoscaler.

    - `scale_down_delay_after_add` - (Defaults to `10m`) How long after scale up that scale down evaluation resumes.

    - `scale_down_unneeded_time` - (Default to `10m`) How long a node should be unneeded before it is eligible for scale down.

    - `estimator` - (Defaults to `binpacking`) Type of resource estimator to be used in scale up.

    - `expander` - (Default to `random`) Type of node group expander to be used in scale up.

    - `ignore_daemonsets_utilization` - (Defaults to `false`) Ignore DaemonSet pods when calculating resource utilization for scaling down.

    - `balance_similar_node_groups` - (Defaults to `false`) Detect similar node groups and balance the number of nodes between them.

    - `expendable_pods_priority_cutoff` - (Defaults to `-10`) Pods with priority below cutoff will be expendable. They can be killed without any consideration during scale down and they don't cause scale up. Pods with null priority (PodPriority disabled) are non expendable.

    - `scale_down_utilization_threshold` - (Defaults to `0.5`) Node utilization level, defined as sum of requested resources divided by capacity, below which a node can be considered for scale down

    - `max_graceful_termination_sec` - (Defaults to `600`) Maximum number of seconds the cluster autoscaler waits for pod termination when trying to scale down a node

- `auto_upgrade` - (Optional) The auto upgrade configuration.

    - `enable` - (Optional) Set to `true` to enable Kubernetes patch version auto upgrades.
~> **Important:** When enabling auto upgrades, the `version` field take a minor version like x.y (ie 1.18).

    - `maintenance_window_start_hour` - (Optional) The start hour (UTC) of the 2-hour auto upgrade maintenance window (0 to 23).

    - `maintenance_window_day` - (Optional) The day of the auto upgrade maintenance window (`monday` to `sunday`, or `any`).

- `feature_gates` - (Optional) The list of [feature gates](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/) to enable on the cluster.

- `admission_plugins` - (Optional) The list of [admission plugins](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/) to enable on the cluster.

- `apiserver_cert_sans` - (Optional) Additional Subject Alternative Names for the Kubernetes API server certificate

- `open_id_connect_config` - (Optional) The OpenID Connect configuration of the cluster

    - `issuer_url` - (Required) URL of the provider which allows the API server to discover public signing keys

    - `client_id` - (Required) A client id that all tokens must be issued for

    - `username_claim` - (Optional) JWT claim to use as the user name

    - `username_prefix` - (Optional) Prefix prepended to username

    - `groups_claim` - (Optional) JWT claim to use as the user's group

    - `groups_prefix` - (Optional) Prefix prepended to group claims

    - `required_claim` - (Optional) Multiple key=value pairs that describes a required claim in the ID Token

- `pod_cidr` - (Optional) The subnet used for the Pod CIDR.

~> **Important:** Changes to this field will recreate a new resource. However once it has been set to a custom value,
unsetting it to go back to the default value will not have any effect.

- `service_cidr` - (Optional) The subnet used for the Service CIDR.

~> **Important:** Changes to this field will recreate a new resource. However once it has been set to a custom value,
unsetting it to go back to the default value will not have any effect.

- `service_dns_ip` - (Optional) The IP used for the DNS Service. If unset, defaults to Service CIDR's network + 10.

~> **Important:** Changes to this field will recreate a new resource. However once it has been set to a custom value,
unsetting it to go back to the default value will not have any effect.

- `region` - (Defaults to [provider](../index.md#arguments-reference) `region`) The [region](../guides/regions_and_zones.md#regions) in which the cluster should be created.

- `project_id` - (Defaults to [provider](../index.md#arguments-reference) `project_id`) The ID of the project the cluster is associated with.

## Attributes Reference

In addition to all arguments above, the following attributes are exported:

- `id` - The ID of the cluster.

~> **Important:** Kubernetes clusters' IDs are [regional](../guides/regions_and_zones.md#resource-ids), which means they are of the form `{region}/{id}`, e.g. `fr-par/11111111-1111-1111-1111-111111111111`

- `created_at` - The creation date of the cluster.
- `updated_at` - The last update date of the cluster.
- `apiserver_url` - The URL of the Kubernetes API server.
- `wildcard_dns` - The DNS wildcard that points to all ready nodes.
- `kubeconfig`
    - `config_file` - The raw kubeconfig file.
    - `host` - The URL of the Kubernetes API server.
    - `cluster_ca_certificate` - The CA certificate of the Kubernetes API server.
    - `token` - The token to connect to the Kubernetes API server.
- `status` - The status of the Kubernetes cluster.
- `upgrade_available` - Set to `true` if a newer Kubernetes version is available.
- `organization_id` - The organization ID the cluster is associated with.

## Import

Kubernetes clusters can be imported using the `{region}/{id}`, e.g.

```bash
terraform import scaleway_k8s_cluster.mycluster fr-par/11111111-1111-1111-1111-111111111111
```

## Deprecation of default_pool

`default_pool` is deprecated in favour the `scaleway_k8s_pool` resource. Here is a migration example.

Before:

```terraform
resource "scaleway_k8s_cluster" "cluster" {
  name    = "tf-cluster"
  version = "1.18.0"
  cni     = "cilium"

  default_pool {
    node_type = "DEV1-M"
    size      = 1
  }
}
```

After:

```terraform
resource "scaleway_k8s_cluster" "cluster" {
  name    = "tf-cluster"
  version = "1.18.0"
  cni     = "cilium"
}

resource "scaleway_k8s_pool" "default" {
  cluster_id = scaleway_k8s_cluster.jack.id
  name       = "default"
  node_type  = "DEV1-M"
  size       = 1
}
```

Once you have moved all the `default_pool` into their own object, you will need to import them. If your pool had the ID 11111111-1111-1111-1111-111111111111 in the `fr-par` region, you can import it by typing:

```bash
terraform import scaleway_k8s_pool.default fr-par/11111111-1111-1111-1111-111111111111
```

Then you will only need to type `terraform apply` to have a smooth migration.
